{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655f11a-3d3e-4e04-929a-7986958f0bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Written and maintained by Andreas Mentzelopoulos\n",
    "Copyright (c) 2025, Andreas Mentzelopoulos. All Rights Reserved.\n",
    "\n",
    "This code is the exclusive property of Andreas Mentzelopoulos\n",
    "All associated materials (data, models, scripts) are the\n",
    "exclusive property of Andreas Mentzelopoulos and LOBSTgER.\n",
    "\n",
    "No part of this code may be copied, distributed, modified, or used in any\n",
    "form without the prior written consent of Andreas Mentzelopoulos.\n",
    "\n",
    "For permission requests, contact: Andreas Mentzelopoulos, ament@mit.edu.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c032c5cd-1287-4ee6-b8b0-b9d39c947a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ament/miniconda3/envs/LOBSTgER/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers.read_data_tools import read_data, imageDataset, latentDataset, build_latentDataset, load_latentDataset, load_imageDataset\n",
    "from helpers.train_utils import read_checkpoint, clean, print_training_logs, write_checkpoint, update_ema, flat_then_decay_w_warmup, update_dataloader\n",
    "from helpers.pre_trained_autoencoder import load_autoencoder\n",
    "from helpers.diffusion_utils_conditional import forward_sample, cosine_beta_schedule, sample_t\n",
    "from helpers.artificial_corruption_utils import corrupt_fast\n",
    "from model_architectures import UnconditionalUNet, ConditionalUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97bad1a2-cb6a-4f25-8b0d-0305027b10f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Favorites: 100%|██████████| 480/480 [00:32<00:00, 14.79it/s]\n",
      "Loading Rest: 100%|██████████| 1992/1992 [02:11<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch encoding for training dataset: 100%|██████████| 78/78 [07:35<00:00,  5.84s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## If you don't store latentDataset and have to read over again\n",
    "H, W = 512, 768 ## Resolution to resize images\n",
    "train_images, train_labels = read_data(H = H, W = W, no_split = True)\n",
    "autoencoder = load_autoencoder(half_precision = True)\n",
    "\n",
    "train_imageDataset = imageDataset(train_images, train_labels)\n",
    "train_latentDataset = build_latentDataset(ImageDataset = train_imageDataset, autoencoder = autoencoder, batch_size = 32)\n",
    "train_loader = DataLoader(train_latentDataset, batch_size = 64, shuffle = True)\n",
    "'''\n",
    "\n",
    "'''\n",
    "train_latentDataset = load_latentDataset()\n",
    "train_loader = DataLoader(train_latentDataset, batch_size = 64, shuffle = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e98f5e-7e09-48ce-a7e2-4566cf1c95ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_imageDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpre_trained_autoencoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m encode, decode \n\u001b[1;32m      3\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m load_autoencoder(half_precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_imageDataset\u001b[49m), (\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m      7\u001b[0m plot_image(train_imageDataset\u001b[38;5;241m.\u001b[39mimages[index])\n\u001b[1;32m      8\u001b[0m plot_image(decode(autoencoder \u001b[38;5;241m=\u001b[39m autoencoder, z \u001b[38;5;241m=\u001b[39m train_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mraw_latents[index]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_imageDataset' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from helpers.plotting_utils import plot_image\n",
    "from helpers.pre_trained_autoencoder import encode, decode \n",
    "autoencoder = load_autoencoder(half_precision = True)\n",
    "\n",
    "index = torch.randint(len(train_imageDataset), (1,))\n",
    "\n",
    "plot_image(train_imageDataset.images[index])\n",
    "plot_image(decode(autoencoder = autoencoder, z = train_loader.dataset.raw_latents[index]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a6d2f7-a9ba-4073-aae0-a8192f5f24d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ament/miniconda3/envs/LOBSTgER/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3482794/1889197993.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2472 train_images.\n",
      "Warning: Checkpoint does not exist, starting from scratch!\n",
      "Loaded train_loader\n",
      "Model number of parameters: 11762604\n",
      "Resuming at epoch: -1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "autoencoder = load_autoencoder(half_precision = True)\n",
    "\n",
    "## Diffusion hyperparameters\n",
    "num_epochs = 1000       # Number of training epochs\n",
    "total_timesteps = 1000   # Number of diffusion timesteps\n",
    "learning_rate = 1e-2     # Learning rate\n",
    "beta_t = cosine_beta_schedule(total_timesteps) # Variance Schedule, cosine or linear\n",
    "cfg_dropout = 0.25       # Dropout for classifier free guidance\n",
    "ema_decay = 0.995        # EMA decay\n",
    "mixed_precision = True   # Mixed precision \n",
    "\n",
    "## Checkpoint\n",
    "log_every = 5\n",
    "checkpoint_every = 100 # epochs\n",
    "\n",
    "## Dataloader augmentation refreshing\n",
    "refresh_dataloader = True\n",
    "refresh_dataloader_every = 10 # epochs\n",
    "refresh_number = 200\n",
    "\n",
    "# Create the Unet model\n",
    "model = ConditionalUNet(dim = 128, dim_mults = (1,2,), beta_t = beta_t, timesteps = total_timesteps).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = flat_then_decay_w_warmup(optimizer = optimizer, num_epochs = num_epochs, flat_until = 0.4, warmup_steps = 100)\n",
    "scaler = GradScaler()\n",
    "\n",
    "## Load the training images\n",
    "train_imageDataset = load_imageDataset()\n",
    "print(f\"Loaded {len(train_imageDataset)} train_images.\")\n",
    "\n",
    "\n",
    "epoch_start, ema_params, losses_total, train_loader = read_checkpoint(model = model, optimizer = optimizer, scaler = scaler, scheduler = scheduler)\n",
    "print(f\"Resuming at epoch: {epoch_start}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae330a04-bc75-4fe0-92c1-40a5ec9a01c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.13 GiB (GPU 0; 31.50 GiB total capacity; 13.39 GiB already allocated; 1.08 GiB free; 13.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mixed_precision:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 28\u001b[0m         v_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m         loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(v_t, v_pred)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss): \u001b[38;5;66;03m# If crash   \u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Latent_UnderWater_Diffusion/model_architectures.py:74\u001b[0m, in \u001b[0;36mConditionalUNet.forward\u001b[0;34m(self, x_noised, x_condition, t)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_noised: torch\u001b[38;5;241m.\u001b[39mTensor, x_condition: torch\u001b[38;5;241m.\u001b[39mTensor, t: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 74\u001b[0m     unet_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_condition\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (B, 8, H, W)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(unet_out)\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:412\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, time, x_self_cond)\u001b[0m\n\u001b[1;32m    409\u001b[0m     x \u001b[38;5;241m=\u001b[39m downsample(x)\n\u001b[1;32m    411\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block1(x, t)\n\u001b[0;32m--> 412\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    413\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block2(x, t)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block1, block2, attn, upsample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mups:\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:265\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    262\u001b[0m mk, mv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: repeat(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh n d -> b h n d\u001b[39m\u001b[38;5;124m'\u001b[39m, b \u001b[38;5;241m=\u001b[39m b), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_kv)\n\u001b[1;32m    263\u001b[0m k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(partial(torch\u001b[38;5;241m.\u001b[39mcat, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), ((mk, k), (mv, v)))\n\u001b[0;32m--> 265\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m out \u001b[38;5;241m=\u001b[39m rearrange(out, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h (x y) d -> b (h d) x y\u001b[39m\u001b[38;5;124m'\u001b[39m, x \u001b[38;5;241m=\u001b[39m h, y \u001b[38;5;241m=\u001b[39m w)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_out(out)\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/denoising_diffusion_pytorch/attend.py:120\u001b[0m, in \u001b[0;36mAttend.forward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m    116\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dropout(attn)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# aggregate values\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb h i j, b h j d -> b h i d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.13 GiB (GPU 0; 31.50 GiB total capacity; 13.39 GiB already allocated; 1.08 GiB free; 13.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "model.train()\n",
    "clean()\n",
    "for epoch in tqdm(range(epoch_start+1, num_epochs)):\n",
    "    \n",
    "    train_losses = []\n",
    "    for i, (batch_conditions, batch_targets, batch_classes) in enumerate(train_loader):\n",
    "\n",
    "        batch_conditions = batch_conditions.half().to(device)\n",
    "        batch_targets = batch_targets.half().to(device)\n",
    "        #batch_classes = batch_classes.to(device)\n",
    "        #print(batch_conditions.shape)\n",
    "        #print(batch_conditions.dtype)\n",
    "        \n",
    "        e = torch.randn_like(batch_targets)\n",
    "        t = sample_t(bad_data = True, TOTAL_TIMESTEPS = total_timesteps, batch_targets = batch_targets, batch_classes = batch_classes)\n",
    "        x_t, v_t = forward_sample(x_0 = batch_targets, t=t, e=e, alphas_cumprod = model.alphas_cumprod) # Calculates velocity\n",
    "\n",
    "        # set to null token for unconditional model\n",
    "        rand_ind = torch.randperm(batch_conditions.shape[0])[:int(cfg_dropout * batch_conditions.shape[0])]\n",
    "        batch_conditions[rand_ind] = model.class_tokens.to(dtype = batch_conditions.dtype)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if mixed_precision:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                v_pred = model(x_t, batch_conditions, t)\n",
    "                loss = torch.nn.functional.mse_loss(v_t, v_pred)\n",
    "             \n",
    "            if torch.isnan(loss): # If crash   \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                continue\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        else:\n",
    "            v_pred = model(x_t, batch_conditions, t)\n",
    "            loss = F.mse_loss(v_t, v_pred)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch > 10:\n",
    "            update_ema(model = model, ema_params = ema_params, ema_decay = ema_decay)\n",
    "        train_losses.append(loss.item())  # Store the original loss\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    losses_total.append(np.mean(train_losses))\n",
    "    \n",
    "    ## User Logs\n",
    "    if (epoch+1) % log_every == 0:\n",
    "        print(f\"Epoch: {epoch+1} Train Loss: {np.mean(train_losses)}\")\n",
    "        print_training_logs(epoch = epoch, losses_total = losses_total, optimizer = optimizer)\n",
    "\n",
    "    ## Write Checkpoint\n",
    "    if (epoch+1) % checkpoint_every == 0 and epoch != 0:\n",
    "        ## Need to update the name of saved model\n",
    "        write_checkpoint(epoch = epoch, model = model, optimizer = optimizer, scaler = scaler, ema_params = ema_params, losses_total = losses_total, train_loader = train_loader,\n",
    "                   scheduler = scheduler)\n",
    "\n",
    "    if refresh_dataloader and (epoch + 1) % refresh_dataloader_every == 0 and epoch != 0:\n",
    "        train_loader =  update_dataloader(train_loader = train_loader, train_imageDataset = train_imageDataset, autoencoder = autoencoder, N = refresh_number)\n",
    "            \n",
    "        #sample_and_save_images(model, epoch+1, num_samples=10, channels=4)\n",
    "        #print(f\"Generated samples at epoch: {epoch+1}\")\n",
    "\n",
    "    if (epoch+1) % 100 == 0 and epoch != 0:\n",
    "        #update_dataloader(train_loader)\n",
    "        #train_loader = renew_dataloader()\n",
    "        clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4d6036-747b-4759-a5fb-3c9ac8013f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, epoch: 290\n"
     ]
    }
   ],
   "source": [
    "write_checkpoint(epoch = epoch, model = model, optimizer = optimizer, scaler = scaler, ema_params = ema_params, losses_total = losses_total, train_loader = train_loader,\n",
    "                   scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6d9f1-3c2b-4407-a9ea-abca68530bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ORCD GPU (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
